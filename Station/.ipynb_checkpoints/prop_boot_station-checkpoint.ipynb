{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preamble\" data-toc-modified-id=\"Preamble-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preamble</a></span></li><li><span><a href=\"#Calculation\" data-toc-modified-id=\"Calculation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Calculation</a></span></li><li><span><a href=\"#Plotting\" data-toc-modified-id=\"Plotting-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Plotting</a></span></li><li><span><a href=\"#Bootstrapping\" data-toc-modified-id=\"Bootstrapping-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bootstrapping</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "''' Packages '''\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# ProgressBar().register()\n",
    "import dask.array\n",
    "import cartopy.crs as ccrs\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "import datetime as dt\n",
    "rb = plt.cm.RdBu\n",
    "bm = plt.cm.Blues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# with warnings.catch_warnings():\n",
    "#     warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_2 = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/Station/Pre xr/MJO_df.pickle'\n",
    "pickle_in_2 = open(path_2, 'rb')\n",
    "MJO_df = pickle.load(pickle_in_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/RMM.pickle'\n",
    "pickle_in = open(path, 'rb')\n",
    "RMM = pickle.load(pickle_in)\n",
    "\n",
    "RMM = RMM.reset_index()\n",
    "RMM = RMM.rename({'Date':'time'}, axis = 1)\n",
    "RMM = RMM.set_index(['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/Station/station_xr.nc'\n",
    "MJO_station = xr.open_mfdataset(path)\n",
    "station_xr = MJO_station.sel(time = slice(RMM.index[0], '2010'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "\n",
    "* 1: Calculate the 90th percentile of each month for each station\n",
    "* 2: Find the evnets greater than the 90th percentile and have a data set of those\n",
    "* 3: Turn this data set into the sum of all the extreme rainfall in each month of each year\n",
    "* 4: Do the same sum for just the normal rainfall\n",
    "* 5: Divide the extreme by the normal; this will be proportation\n",
    "* 6: Now do the same thing but for each phase of the MJO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMM_active = RMM[RMM['Amplitude'] >= 1]\n",
    "RMM_inactive = np.array(RMM[RMM['Amplitude'] < 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mjo_phase_data(data_sub, enhanced_phases):\n",
    "    enhanced_dates = np.array(RMM_active[RMM_active['Phase'].isin(enhanced_phases)].index)\n",
    "    suppressed_dates = np.array(RMM_active[~RMM_active['Phase'].isin(enhanced_phases)].index)\n",
    "    \n",
    "    enhanced_data = data_sub.where(data_sub.time.isin(enhanced_dates))\n",
    "    suppressed_data = data_sub.where(data_sub.time.isin(suppressed_dates))\n",
    "    inactive_data = data_sub.where(data_sub.time.isin(RMM_inactive))\n",
    "\n",
    "    return enhanced_data, suppressed_data, inactive_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_phase_function(lon):\n",
    "    if lon < 120:\n",
    "        enhanced_phases = [4,5]\n",
    "    if lon >= 120 and lon <140:\n",
    "        enhanced_phases = [4,5,6]\n",
    "    else:\n",
    "        enhanced_phases = [4,5,6,7]\n",
    "        \n",
    "    return enhanced_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_xr(storage):\n",
    "    \n",
    "    xr_file = xr.Dataset({'precip':(('location','month'),np.array(storage)),\n",
    "                  'lat':(('location'), np.array(MJO_df.Lat).astype(float)),\n",
    "                  'lon':(('location'), np.array(MJO_df.Lon).astype(float))\n",
    "                  },\n",
    "                 {'location':station_xr.location.values, 'month':[10,11,12,1,2,3]})\n",
    "    \n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_surplus_data(xr_file):\n",
    "    xr_file = xr_file.drop('lat')\n",
    "    xr_file = xr_file.drop('lon')\n",
    "    xr_file = xr_file.drop('station_name')\n",
    "    \n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function find the extremes for a specific station. THe a\n",
    "\n",
    "def return_extremes(data, q):\n",
    "    \n",
    "    # Finding the extreme threshhold for each of the months\n",
    "    data_exthresh = data.precip.groupby('time.month').reduce(np.nanpercentile,q= q, dim = 'time')\n",
    "    \n",
    "    \n",
    "    # Now going through and finding all of the extremes for each month\n",
    "    months = [10,11,12,1,2,3]\n",
    "    ex_storage = []\n",
    "    \n",
    "    for month in months:\n",
    "        \n",
    "        # This is just the data for a single month\n",
    "        data_m = data.where(data.time.dt.month == month, drop = True)\n",
    "        threshold = data_exthresh.sel(month = 1)\n",
    "        data_ex  = data_m.where(data_m.precip >= threshold)\n",
    "        ex_storage.append(data_ex)\n",
    "        \n",
    "    xr_file = xr.concat(ex_storage, pd.Index(months, name = 'month'))\n",
    "    \n",
    "    \n",
    "    xr_file = drop_surplus_data(xr_file)\n",
    "    \n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_month_prop(data,data_ex):\n",
    "    # The data_ex has been divided into months, but the data has not.\n",
    "    \n",
    "    data_sum = data.precip.groupby('time.month').sum(dim = 'time')\n",
    "    data_ex_sum = data_ex.precip.groupby('month').sum(dim = 'time')\n",
    "    \n",
    "    prop = data_ex_sum/data_sum\n",
    "    \n",
    "    return prop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_station_prop(data, q):\n",
    "    # For loops that goes through all stations\n",
    "    enhanced_storage = []\n",
    "    suppressed_storage = []\n",
    "    inactive_storage = []\n",
    "\n",
    "    for station in np.array([data.location.values])[0]:\n",
    "\n",
    "        # Getting the data just from these years\n",
    "        data_sub = data.sel(location = station)\n",
    "        data_sub_ex = return_extremes(data_sub, q) #An array of the extreme values\n",
    "\n",
    "        # Finding the enahnced phases:\n",
    "        lon = station_xr.sel(location = station).lon.values\n",
    "        enhanced_phases = enhanced_phase_function(lon)\n",
    "\n",
    "        # Divide up into enhanced, suppressed and inactive\n",
    "        enhanced, suppressed, inactive = mjo_phase_data(data_sub, enhanced_phases)\n",
    "        \n",
    "        # Finding the extreme of these phases\n",
    "        \n",
    "        ehnaced_ex = return_extremes(enhanced, q)\n",
    "        suppressed_ex = return_extremes(suppressed, q)\n",
    "        inactive_ex = return_extremes(inactive, q)\n",
    "        \n",
    "\n",
    "        # Calculate 90th percentile for each phase in each month\n",
    "        enhanced_q = calculate_month_prop(enhanced,ehnaced_ex)\n",
    "        suppressed_q = calculate_month_prop(suppressed,suppressed_ex)\n",
    "        inactive_q = calculate_month_prop(inactive,inactive_ex)\n",
    "\n",
    "        normal_q = calculate_month_prop(data_sub, data_sub_ex)\n",
    "\n",
    "        # Do the Frac\n",
    "        enhanced_frac = enhanced_q/normal_q\n",
    "        suppressed_frac = suppressed_q/ normal_q\n",
    "        inacitve_frac = inactive_q/ normal_q\n",
    "\n",
    "        # Storing all the values in array\n",
    "        enhanced_storage.append(np.array(enhanced_frac.values))\n",
    "        suppressed_storage.append(np.array(suppressed_frac.values))\n",
    "        inactive_storage.append(np.array(inacitve_frac.values))\n",
    "\n",
    "    # Adding them all to xarray files with lon and lat readded\n",
    "\n",
    "    enhanced_frac = into_xr(enhanced_storage)\n",
    "    suppressed_frac = into_xr(suppressed_storage)\n",
    "    inactive_frac = into_xr(inactive_storage)\n",
    "    \n",
    "    total_frac = xr.concat([enhanced_frac, suppressed_frac, inactive_frac], pd.Index(['enhanced',\n",
    "                                                                                     'suppressed','inactive'],\n",
    "                                                                                    name = 'mjo'))\n",
    "    \n",
    "    return total_frac\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = station_xr\n",
    "q = 90\n",
    "\n",
    "station_prop_90 = calculate_station_prop(data, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = station_xr\n",
    "q = 95\n",
    "\n",
    "station_prop_95 = calculate_station_prop(data, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = 0\n",
    "\n",
    "save_dr = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/Station/prop_data/'\n",
    "if save:\n",
    "    station_prop_90.to_netcdf(save_dr + 'station_prop_90.nc')\n",
    "    station_prop_90.to_netcdf(save_dr +'station_prop_95.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MidpointNormalizeLog(colors.LogNorm):\n",
    "    def __init__(self, vmin, vmax, midpoint):\n",
    "        self.midpoint = midpoint\n",
    "        super().__init__(vmin=vmin, vmax=vmax)\n",
    "        \n",
    "    def __call__(self, value, clip=None):\n",
    "        original = super().__call__(value, clip)\n",
    "        procmid = super().__call__(self.midpoint, clip)\n",
    "        \n",
    "        x, y = [0, procmid, 1], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(original, x, y), np.isnan(original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize  = (15,8), dpi = 400)\n",
    "ax = fig.add_subplot(1,1,1,projection = ccrs.PlateCarree())\n",
    "\n",
    "data = station_prop_90.sel(mjo = 'enhanced',month = 12)\n",
    "vmax = 2.5\n",
    "vmin = 0.5\n",
    "\n",
    "plot = plt.scatter(data.lon.values,data.lat, s= 40,c = data.precip, cmap = rb,\n",
    "            norm =  MidpointNormalizeLog(vmin = vmin, vmax = vmax, midpoint=1))\n",
    "plt.colorbar(plot)\n",
    "\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize  = (15,8), dpi = 400)\n",
    "ax = fig.add_subplot(1,1,1,projection = ccrs.PlateCarree())\n",
    "\n",
    "data = station_prop_90.sel(mjo = 'suppressed',month = 12)\n",
    "vmax = 2.5\n",
    "vmin = 0.5\n",
    "\n",
    "plot = plt.scatter(data.lon.values,data.lat, s= 40,c = data.precip, cmap = rb,\n",
    "            norm =  MidpointNormalizeLog(vmin = vmin, vmax = vmax, midpoint=1))\n",
    "plt.colorbar(plot)\n",
    "\n",
    "ax.coastlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMM_active = RMM[RMM['Amplitude'] >= 1]\n",
    "RMM_inactive = np.array(RMM[RMM['Amplitude'] < 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data, suppressed_data, inactive_data = mjo_phase_data(station_xr.isel(location = 11), [4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mjo_phase_data(data_sub, enhanced_phases):\n",
    "    enhanced_dates = np.array(RMM_active[RMM_active['Phase'].isin(enhanced_phases)].index)\n",
    "    suppressed_dates = np.array(RMM_active[~RMM_active['Phase'].isin(enhanced_phases)].index)\n",
    "    \n",
    "    enhanced_data = data_sub.where(data_sub.time.isin(enhanced_dates))\n",
    "    suppressed_data = data_sub.where(data_sub.time.isin(suppressed_dates))\n",
    "    inactive_data = data_sub.where(data_sub.time.isin(RMM_inactive))\n",
    "\n",
    "    return enhanced_data, suppressed_data, inactive_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_phase_function(lon):\n",
    "    if lon < 120:\n",
    "        enhanced_phases = [4,5]\n",
    "    if lon >= 120 and lon <140:\n",
    "        enhanced_phases = [4,5,6]\n",
    "    else:\n",
    "        enhanced_phases = [4,5,6,7]\n",
    "        \n",
    "    return enhanced_phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_xr_bs(storage):\n",
    "    \n",
    "    xr_file = xr.Dataset({'frac':(('location','month'),np.array(storage))},\n",
    "                 {'location':station_xr.location.values, 'month':[10,11,12,1,2,3]})\n",
    "    \n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just moves all the data number * 100 years into the futre\n",
    "\n",
    "def forward_in_time(additional, number):\n",
    "    times = ((additional.time.dt.year.values + number *100 )* 10000 + \n",
    "                       additional.time.dt.month.values *100 + additional.time.dt.day.values)\n",
    "    values = additional.precip.values\n",
    "    \n",
    "    future_time = []\n",
    "    future_values = []\n",
    "    \n",
    "    for i in range(len(times)):\n",
    "        time = times[i]\n",
    "        try:\n",
    "            thetime = pd.to_datetime(time,format = '%Y%m%d')\n",
    "            future_time.append(thetime)\n",
    "            future_values.append(values[i])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "    xrd = xr.Dataset({'precip':(('time'), future_values)},\n",
    "                {'time':future_time})\n",
    "\n",
    "    return xrd\n",
    "    \n",
    "\n",
    "\n",
    "# * Contains 1 additional function \n",
    "def select_additional(data, years):\n",
    "    \n",
    "    number = 1\n",
    "    uniqueValues, occurCount = np.unique(years, return_counts=True) \n",
    "    \n",
    "    while(any(occurCount >= 2)):\n",
    "        # Find the years that are getting sampled multiple times\n",
    "        multi_sampled_years = uniqueValues[np.where(occurCount >= 2 )]\n",
    "        \n",
    "        additional = data.where(data.time.dt.year.isin(multi_sampled_years), drop = True)\n",
    "        \n",
    "\n",
    "        additional = forward_in_time(additional, number)\n",
    "               \n",
    "        data = data.combine_first(additional)\n",
    "                                \n",
    "        occurCount = occurCount - 1\n",
    "        number += 1\n",
    "        \n",
    "    \n",
    "    return data\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function find the extremes for a specific station. THe a\n",
    "\n",
    "def return_extremes_bs(data, q):\n",
    "    \n",
    "    # Finding the extreme threshhold for each of the months\n",
    "    data_exthresh = data.precip.groupby('time.month').reduce(np.nanpercentile,q= q, dim = 'time')\n",
    "    \n",
    "    \n",
    "    # Now going through and finding all of the extremes for each month\n",
    "    months = [10,11,12,1,2,3]\n",
    "    ex_storage = []\n",
    "    \n",
    "    for month in months:\n",
    "        \n",
    "        # This is just the data for a single month\n",
    "        data_m = data.where(data.time.dt.month == month, drop = True)\n",
    "        threshold = data_exthresh.sel(month = 1)\n",
    "        data_ex  = data_m.where(data_m.precip >= threshold)\n",
    "        ex_storage.append(data_ex)\n",
    "        \n",
    "    xr_file = (ex_storage[0].combine_first(ex_storage[1]).combine_first(ex_storage[2])\n",
    "               .combine_first(ex_storage[3]).combine_first(ex_storage[4]).combine_first(ex_storage[5]))\n",
    "    \n",
    "    xr_file = drop_surplus_data(xr_file)\n",
    "    \n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_month_prop_bs(data,data_ex):\n",
    "    # The data_ex has been divided into months, but the data has not.\n",
    "    \n",
    "    data_sum = data.precip.groupby('time.month').sum(dim = 'time')\n",
    "    data_ex_sum = data_ex.precip.groupby('time.month').sum(dim = 'time')\n",
    "    \n",
    "    prop = data_ex_sum/data_sum\n",
    "    \n",
    "    return prop\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def calculate_station_prop_bs(data, q, loops):\n",
    "    # For loops that goes through all stations\n",
    "    enhanced_storage_10 = []\n",
    "    suppressed_storage_10 = []\n",
    "    inactive_storage_10 = []\n",
    "    \n",
    "    enhanced_storage_90 = []\n",
    "    suppressed_storage_90 = []\n",
    "    inactive_storage_90 = []\n",
    "    \n",
    "    years = np.arange(1974,2011)\n",
    "    \n",
    "    stations = np.array([data.location.values])[0]\n",
    "    for station in stations:\n",
    "\n",
    "        # Getting the data just from these years\n",
    "        data_sub = data.sel(location = station)\n",
    "        data_sub_ex = return_extremes_bs(data_sub, q) #An array of the extreme values\n",
    "\n",
    "        # Finding the enahnced phases:\n",
    "        lon = station_xr.sel(location = station).lon.values\n",
    "        enhanced_phases = enhanced_phase_function(lon)\n",
    "        \n",
    "\n",
    "        # Divide up into enhanced, suppressed and inactive\n",
    "        enhanced, suppressed, inactive = mjo_phase_data(data_sub, enhanced_phases)\n",
    "        \n",
    "        \n",
    "        # Finding the extreme of these phases\n",
    "        enhanced_ex = return_extremes_bs(enhanced, q)\n",
    "        suppressed_ex = return_extremes_bs(suppressed, q)\n",
    "        inactive_ex = return_extremes_bs(inactive, q)\n",
    "\n",
    "        # Calculate 90th percentile for each phase in each month\n",
    "        enhanced_run_storage = []\n",
    "        suppressed_run_storage = []\n",
    "        inactive_run_storage = []\n",
    "        \n",
    "        for run in range(loops):\n",
    "    \n",
    "            # Get the good years for that station\n",
    "            bad_years = MJO_df.loc[station].loc['Bad Wet Seasons']\n",
    "            good_years = years[~np.isin(years,np.array(bad_years))]\n",
    "            \n",
    "            # Random years\n",
    "            random_years = np.random.choice(good_years,\n",
    "                                            size = np.ceil(len(good_years)/2).astype(int), replace = True )\n",
    "            \n",
    "            ####### Random of the Normal Phase Rain\n",
    "            # Getting the data just from these years\n",
    "            enhanced_rand = enhanced.where(enhanced.time.dt.year.isin(random_years), drop = True)\n",
    "            suppressed_rand = suppressed.where(suppressed.time.dt.year.isin(random_years), drop = True)\n",
    "            inactive_rand = inactive.where(inactive.time.dt.year.isin(random_years), drop = True)\n",
    "            normal_rand = data_sub.where(data_sub.time.dt.year.isin(random_years), drop = True)\n",
    "            \n",
    "        \n",
    "            # Dealing with the years sample multiple time\n",
    "            enhanced_rand = select_additional(enhanced_rand, random_years)\n",
    "            suppressed_rand = select_additional(suppressed_rand, random_years)\n",
    "            inactive_rand = select_additional(inactive_rand, random_years)\n",
    "            normal_rand = select_additional(normal_rand, random_years)\n",
    "            \n",
    "            \n",
    "            ####### Random of the Extreme Phase Rain\n",
    "            # Getting the data just from these years\n",
    "            enhanced_rand_ex = enhanced_ex.where(enhanced_ex.time.dt.year.isin(random_years), drop = True)\n",
    "            suppressed_rand_ex = suppressed_ex.where(suppressed_ex.time.dt.year.isin(random_years), drop = True)\n",
    "            inactive_rand_ex = inactive_ex.where(inactive_ex.time.dt.year.isin(random_years), drop = True)\n",
    "            normal_rand_ex = data_sub_ex.where(data_sub_ex.time.dt.year.isin(random_years), drop = True)\n",
    "            \n",
    "        \n",
    "\n",
    "            #Dealing with the years sample multiple time\n",
    "            enhanced_rand_ex = select_additional(enhanced_rand_ex, random_years)\n",
    "            suppressed_rand_ex = select_additional(suppressed_rand_ex, random_years)\n",
    "            inactive_rand_ex = select_additional(inactive_rand_ex, random_years)\n",
    "            normal_rand_ex = select_additional(normal_rand_ex, random_years)\n",
    "            \n",
    "            \n",
    "\n",
    "            ########Actual Calculation\n",
    "            enhanced_q = calculate_month_prop_bs(enhanced_rand,enhanced_rand_ex)\n",
    "            suppressed_q = calculate_month_prop_bs(suppressed_rand,suppressed_rand_ex)\n",
    "            inactive_q = calculate_month_prop_bs(inactive_rand, inactive_rand_ex)\n",
    "            normal_q = calculate_month_prop_bs(normal_rand, normal_rand_ex)\n",
    "\n",
    "            # Do the Frac\n",
    "            enhanced_frac = enhanced_q/normal_q\n",
    "            suppressed_frac = suppressed_q/ normal_q\n",
    "            inactive_frac = inactive_q/ normal_q\n",
    "            \n",
    "            \n",
    "            #Storing the Results\n",
    "            enhanced_run_storage.append(enhanced_frac)\n",
    "            suppressed_run_storage.append(suppressed_frac)\n",
    "            inactive_run_storage.append(inactive_frac)\n",
    "\n",
    "        # Concatinating all the storages\n",
    "        enhanced_total = xr.concat(enhanced_run_storage,  'run_num')\n",
    "        suppressed_total = xr.concat(suppressed_run_storage,  'run_num')\n",
    "        inactive_total = xr.concat(inactive_run_storage,  'run_num')\n",
    "        \n",
    "        \n",
    "        #Calculating the 10th and 90th percentile\n",
    "        enhanced_90 = enhanced_total.groupby('month').reduce(np.nanpercentile, q = 90)\n",
    "        enhanced_10 = enhanced_total.groupby('month').reduce(np.nanpercentile, q = 10)\n",
    "        \n",
    "        suppressed_90 = suppressed_total.groupby('month').reduce(np.nanpercentile, q = 90)\n",
    "        suppressed_10 = suppressed_total.groupby('month').reduce(np.nanpercentile, q = 10)\n",
    "        \n",
    "        inactive_90 = inactive_total.groupby('month').reduce(np.nanpercentile, q = 90)\n",
    "        inactive_10 = inactive_total.groupby('month').reduce(np.nanpercentile, q = 10)\n",
    "\n",
    "\n",
    "        # Storing all of 10th and 90th percentiles of run numbs\n",
    "        enhanced_storage_90.append(enhanced_90)\n",
    "        enhanced_storage_10.append(enhanced_10)\n",
    "        \n",
    "        suppressed_storage_90.append(suppressed_90)\n",
    "        suppressed_storage_10.append(suppressed_10)\n",
    "        \n",
    "        inactive_storage_90.append(inactive_90)\n",
    "        inactive_storage_10.append(inactive_10)\n",
    "        \n",
    "                                    \n",
    "                                    \n",
    "    # The concatination of all the stations\n",
    "    enhanced_tot_90 = xr.concat(enhanced_storage_90, pd.Index(stations, name = 'location'))\n",
    "    enhanced_tot_10 = xr.concat(enhanced_storage_10, pd.Index(stations, name = 'location'))\n",
    "    enhanced_tot = xr.concat([enhanced_tot_90,enhanced_tot_10], pd.Index(['90','10'], name = 'percentile'))\n",
    "    \n",
    "    suppressed_tot_90 = xr.concat(suppressed_storage_90, pd.Index(stations, name = 'location'))\n",
    "    suppressed_tot_10 = xr.concat(suppressed_storage_10, pd.Index(stations, name = 'location'))\n",
    "    suppressed_tot = xr.concat([suppressed_tot_90,suppressed_tot_10], pd.Index(['90','10'], name = 'percentile'))\n",
    "    \n",
    "    inactive_tot_90 = xr.concat(inactive_storage_90, pd.Index(stations, name = 'location'))\n",
    "    inactive_tot_10 = xr.concat(inactive_storage_10, pd.Index(stations, name = 'location'))\n",
    "    inactive_tot = xr.concat([inactive_tot_90,inactive_tot_10], pd.Index(['90','10'], name = 'percentile'))\n",
    "    \n",
    "    return enhanced_tot,suppressed_tot,inactive_tot\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = station_xr\n",
    "q = 90\n",
    "loops = 1000\n",
    "enhanced_90,suppressed_90,inactive_90 = calculate_station_prop_bs(data, q, loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_prop_boot_90 = xr.concat([enhanced_90,suppressed_90],\n",
    "                             pd.Index(['enhanced', 'suppressed'], name = 'mjo'))\n",
    "\n",
    "save_dr = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/Station/prop_data/'\n",
    "stat_prop_boot_90.to_netcdf(save_dr + 'station_prop_boot_90_2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = station_xr\n",
    "q = 95\n",
    "loops = 1000\n",
    "enhanced_95,suppressed_95,inactive_95 = calculate_station_prop_bs(data, q, loops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_prop_boot_95 = xr.concat([enhanced_95,suppressed_95],\n",
    "                             pd.Index(['enhanced', 'suppressed'], name = 'mjo'))\n",
    "save_dr = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/MJO/Station/prop_data/'\n",
    "stat_prop_boot_95.to_netcdf(save_dr + 'station_prop_boot_95_2.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "771px",
    "left": "26px",
    "top": "111px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
