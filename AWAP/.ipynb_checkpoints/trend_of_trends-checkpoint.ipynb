{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import dask.array\n",
    "import cartopy.crs as ccrs\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "import datetime as dt\n",
    "rb = plt.cm.RdBu\n",
    "bm = plt.cm.Blues\n",
    "best_blue = '#9bc2d5'\n",
    "recherche_red = '#fbc4aa'\n",
    "wondeful_white = '#f8f8f7'\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "path = 'RMM.pickle'\n",
    "pickle_in = open(path, 'rb')\n",
    "RMM = pickle.load(pickle_in)\n",
    "\n",
    "RMM = RMM.reset_index()\n",
    "RMM['Date'] = RMM['Date'] + pd.to_timedelta('9h')\n",
    "RMM = RMM.set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %load /home/563/ab2313/MJO/get_awap.py\n",
    "import sys\n",
    "\n",
    "def get_platform():\n",
    "    platforms = {\n",
    "        'linux1' : 'Linux',\n",
    "        'linux2' : 'Linux',\n",
    "        'darwin' : 'OS X',\n",
    "        'win32' : 'Windows'\n",
    "    }\n",
    "\n",
    "    if sys.platform not in platforms:\n",
    "        return sys.platform\n",
    "\n",
    "    return platforms[sys.platform]\n",
    "\n",
    "\n",
    "platform = get_platform()\n",
    "\n",
    "\n",
    "if platform == 'OS X':\n",
    "    path =  '/Users/alexborowiak/Desktop/large_files/'\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "else:\n",
    "#     path = '/home/student.unimelb.edu.au/aborowiak/Desktop/Code/Scripts/big_files/'\n",
    "    path = '/home/563/ab2313/big_files/'\n",
    "\n",
    "\n",
    "# precip = xr.open_dataset(path + 'AWAP_w.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "precip = xr.open_dataset(path + 'AWAP_W.nc', chunks={'time':-1, 'lat': 50, 'lon': 50}).precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd() + '/'\n",
    "\n",
    "save_dir = cwd + 'trend_plots/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/563/ab2313/MJO/AWAP/trend_data/'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_data_dir = cwd + 'trend_data/'\n",
    "save_data_dir "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ------Different Regions\n",
    "\n",
    "The rainfall is divided into different phases\n",
    "\n",
    "Returns:\n",
    "* <b> phase_precip </b> All the rainfall data split into the MJO phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# This is creating 3 seperate data frames comprised of the rainfall just in the individual phases\n",
    "\n",
    "\n",
    "def split_via_mjo(data, RMM):\n",
    "    \n",
    "    # The different regions and which phases are enhanced\n",
    "    regions = np.array([slice(110, 120),slice(120.25, 140),slice(140.25, 156.25)])\n",
    "    mjo_enhanced_list = np.array([[4,5],[4,5,6],[4,5,6,7]])\n",
    "    \n",
    "    \n",
    "    \n",
    "    enhanced_data = []\n",
    "    suppressed_data = []\n",
    "    inactive_data = []\n",
    "\n",
    "    # Three different regins to loop through: west, middle and east\n",
    "    for reg_num in [0,1,2]:\n",
    "\n",
    "        # Selecting just the values in that region\n",
    "        region = regions[reg_num]\n",
    "        data_region = data.sel(lon = region)\n",
    "\n",
    "        # Selecting the phases that are enhanced and suppressed for that region\n",
    "        mjo_enhanced = mjo_enhanced_list[reg_num] #enhanced\n",
    "        mjo_suppressed = mjo_enhanced_list[reg_num] # suppressed\n",
    "\n",
    "\n",
    "        # Getting just the dates for that phase: has to be in phases and have RMM >= 1\n",
    "        enhanced_dates = np.array(RMM[np.logical_and(RMM['Phase'].isin(mjo_enhanced), \n",
    "                                                     RMM['Amplitude'] >= 1)].index)\n",
    "\n",
    "        suppressed_dates = np.array(RMM[np.logical_and(~RMM['Phase'].isin(mjo_suppressed), \n",
    "                                                       RMM['Amplitude'] >= 1)].index)\n",
    "\n",
    "        inactive_dates = np.array(RMM[RMM['Amplitude'] < 1].index)\n",
    "\n",
    "\n",
    "\n",
    "        # Now getting an xarray file for the values just in that phase\n",
    "        data_enhanced = data_region.where(data_region.time.isin(enhanced_dates))\n",
    "        data_suppressed = data_region.where(data_region.time.isin(suppressed_dates))\n",
    "        data_inactive  = data_region.where(data_region.time.isin(inactive_dates))\n",
    "\n",
    "\n",
    "        # Appending to a list\n",
    "        enhanced_data.append(data_enhanced)\n",
    "        suppressed_data.append(data_suppressed)\n",
    "        inactive_data.append(data_inactive)\n",
    "\n",
    "\n",
    "\n",
    "    # Putting into xarray file\n",
    "    enhanced_data = (enhanced_data[0].combine_first(enhanced_data[1])).combine_first(enhanced_data[2])\n",
    "    suppressed_data = (suppressed_data[0].combine_first(suppressed_data[1])).combine_first(suppressed_data[2])\n",
    "    inactive_data = (inactive_data[0].combine_first(inactive_data[1])).combine_first(inactive_data[2])\n",
    "\n",
    "    # Total Xarray File\n",
    "\n",
    "\n",
    "    xr_file = xr.concat([enhanced_data, suppressed_data, inactive_data, data]\n",
    "                        , pd.Index(['enhanced','suppressed','inactive','all' ], name = 'mjo'))\n",
    "\n",
    "    return xr_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "phase_precip = split_via_mjo(precip, RMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This returns just the extremes. \n",
    "\n",
    "def return_extremes(precip, threshold):\n",
    "    storage = []\n",
    "    months = [10,11,12,1,2,3]\n",
    "    for i,month in enumerate(months):\n",
    "        precip_month = precip.where(precip.time.dt.month == month, drop = True)\n",
    "        threshold_month = threshold.sel(month = month)\n",
    "        \n",
    "        precip_month_ex = precip_month.where(precip_month >= threshold_month)\n",
    "\n",
    "        storage.append(precip_month_ex)\n",
    "        \n",
    "        if i == 0:\n",
    "            extreme_xr = precip_month_ex\n",
    "        else:\n",
    "            extreme_xr = extreme_xr.combine_first(precip_month_ex)  \n",
    "    \n",
    "    return  extreme_xr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": [
     21,
     37,
     70,
     85
    ]
   },
   "outputs": [],
   "source": [
    "'''Mann-Kendall'''\n",
    "# All of these functions feed into the bottom to calculate the p-value for the mann-kendall test. Notes\n",
    "# on this can be found in section 2 of your hand-written notes \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Someone else code\n",
    "# https://github.com/mps9506/Mann-Kendall-Trend/blob/master/mk_test.py\n",
    "\n",
    "\n",
    "# Guide\n",
    "# https://vsp.pnnl.gov/help/Vsample/Design_Trend_Mann_Kendall.htm\n",
    "\n",
    "# Paper on the test that was referenced in Andrews text book\n",
    "# https://pdf-sciencedirectassets-com.ezp.lib.unimelb.edu.au/271842/1-s2.0-S0022169409X00028/1\n",
    "# -s2.0-S0022169408005787/main.pdf?X-Amz-Security-Token=AgoJb3JpZ2luX2VjEEEaCXVzLWVhc3QtMSJHMEUCIQC%2BgafAjE%2BA8EnQXkkXF8LhgWx76TCsGWjWRHRbeXLjRQIgcPemLsgQSvPUUC4xfLuliC4DrQMz9MmOKetDGb5xNdQq4wMImv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDOgtXZ3v3oZsI2BNfCq3A%2F7PIeAEoR8mS7kPthCx41EgUIo4j4021XWkezn6g2E5PvE1Sz3floMw8iWkKhvFJBRVofbbBKUWu%2BUVFPR4o28J7LHcX897I%2FUWKp15jOYOGomooZFcGw%2Fr0XnHRlKfdpd0NEdaIpejd9pTErS8%2FI4H%2Bf6aqIYsQroIq6%2Bf1yapaJzpasC9E6bzAdBsD84kUWGFPSL%2F7FDBn03ZwOIPZXMekc9GVBdj2oYE9PPSQqrYZfPR2hhDB61EacmT7%2B7Eqrl2AKmkAMYeXp7MJeuENwAem5QVHafb6uaTnyH2yBm6D1v50%2FUjft3OQnHU4RpvoyF3HEGL8WjiWXW%2FHPSG1oVOgbo9A0WTSVJivrXk9b9w%2FjcISXxRZl8Ptf9Mq1%2BzFHQO4JUolrkPhiFnmUedhpwWfRz%2BbVW3hJuloOR1kI%2BLiw9w9kthWdH3Chn2CBKIC3P26%2FpNAxbNNAsUreqwr8JkNObN6s0zcDaAGpUrMbCl1HCosVyfEi4f9LmQCRrt09Z7Q95LQrRWkIz%2BZ3V%2B3QzXsxrVJM%2FY9JBVdSErwHl0u3poxo7aasLdyhWdEgzZNnyDEOYsFbEw%2BYea6QU6tAEobaIGSgYpHHhX7hKQ%2FfKb4I2GYkBASK7DQDyzoVQmnAuPXv1T1x%2FSfwDpcbCQky3VgKDWPqKf%2B1sifCSy6G7kU4nhqHCGCmRAkEjrQ0wJPODAyBorXCZtkJtpYzJYymg5bAOpj1G1VtGh9b52UGMmZrM%2F4DPt%2FXiv8aLSpWetDhZRvu7isewzafIxRf1v7MFUWmy2IxcYc%2B09qv4vQ2hz1eJmdDGSK3BJX5Tmwo1P1WsJu4E%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20190711T012019Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7BKKOEEP%2F20190711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d0e4e40feb1ef7c74c1a0a992e0016cdca18e24a6673e3f45f5011d0d36c9e4b&hash=67fd743a3b4a4d7fff889b5960a8e1506cdf7ae726332c3884fc00cb9e489047&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0022169408005787&tid=spdf-2f5713d9-f87d-42a0-88bc-015c4c6897be&sid=74c57e9c7e5215466838746-2595860e9bd7gxrqa&type=clienthttps://pdf-sciencedirectassets-com.ezp.lib.unimelb.edu.au/271842/1-s2.0-S0022169409X00028/1-s2.0-S0022169408005787/main.pdf?X-Amz-Security-Token=AgoJb3JpZ2luX2VjEEEaCXVzLWVhc3QtMSJHMEUCIQC%2BgafAjE%2BA8EnQXkkXF8LhgWx76TCsGWjWRHRbeXLjRQIgcPemLsgQSvPUUC4xfLuliC4DrQMz9MmOKetDGb5xNdQq4wMImv%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDOgtXZ3v3oZsI2BNfCq3A%2F7PIeAEoR8mS7kPthCx41EgUIo4j4021XWkezn6g2E5PvE1Sz3floMw8iWkKhvFJBRVofbbBKUWu%2BUVFPR4o28J7LHcX897I%2FUWKp15jOYOGomooZFcGw%2Fr0XnHRlKfdpd0NEdaIpejd9pTErS8%2FI4H%2Bf6aqIYsQroIq6%2Bf1yapaJzpasC9E6bzAdBsD84kUWGFPSL%2F7FDBn03ZwOIPZXMekc9GVBdj2oYE9PPSQqrYZfPR2hhDB61EacmT7%2B7Eqrl2AKmkAMYeXp7MJeuENwAem5QVHafb6uaTnyH2yBm6D1v50%2FUjft3OQnHU4RpvoyF3HEGL8WjiWXW%2FHPSG1oVOgbo9A0WTSVJivrXk9b9w%2FjcISXxRZl8Ptf9Mq1%2BzFHQO4JUolrkPhiFnmUedhpwWfRz%2BbVW3hJuloOR1kI%2BLiw9w9kthWdH3Chn2CBKIC3P26%2FpNAxbNNAsUreqwr8JkNObN6s0zcDaAGpUrMbCl1HCosVyfEi4f9LmQCRrt09Z7Q95LQrRWkIz%2BZ3V%2B3QzXsxrVJM%2FY9JBVdSErwHl0u3poxo7aasLdyhWdEgzZNnyDEOYsFbEw%2BYea6QU6tAEobaIGSgYpHHhX7hKQ%2FfKb4I2GYkBASK7DQDyzoVQmnAuPXv1T1x%2FSfwDpcbCQky3VgKDWPqKf%2B1sifCSy6G7kU4nhqHCGCmRAkEjrQ0wJPODAyBorXCZtkJtpYzJYymg5bAOpj1G1VtGh9b52UGMmZrM%2F4DPt%2FXiv8aLSpWetDhZRvu7isewzafIxRf1v7MFUWmy2IxcYc%2B09qv4vQ2hz1eJmdDGSK3BJX5Tmwo1P1WsJu4E%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20190711T012019Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY7BKKOEEP%2F20190711%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=d0e4e40feb1ef7c74c1a0a992e0016cdca18e24a6673e3f45f5011d0d36c9e4b&hash=67fd743a3b4a4d7fff889b5960a8e1506cdf7ae726332c3884fc00cb9e489047&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0022169408005787&tid=spdf-2f5713d9-f87d-42a0-88bc-015c4c6897be&sid=74c57e9c7e5215466838746-2595860e9bd7gxrqa&type=client\n",
    "\n",
    "\n",
    "def S_kendall(data):\n",
    "    S = 0\n",
    "    n = len(data)\n",
    "    \n",
    "    # Outer sum\n",
    "    for i in np.arange(0, n - 1):\n",
    "        # Inner sum\n",
    "        for k in np.arange(i + 1, n):\n",
    "\n",
    "            S += np.sign(data[k]  - data[i])\n",
    "\n",
    "    return S\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "def var_kendall(data):\n",
    "    unique_vals, count_vals = np.unique(data, return_counts = True)\n",
    "    \n",
    "    n = len(data)\n",
    "    var = n*(n - 1)* (2*n +5)/ 18\n",
    "    \n",
    "    ######\n",
    "    if any(count_vals > 1): # there is a repated value\n",
    "        \n",
    "        # These are the number of samples that have been used multiple times\n",
    "        multi_sampled = count_vals[np.where(count_vals > 1)] \n",
    "        \n",
    "        # This following is doing the sum as seen in the varience equation\n",
    "        summed = 0\n",
    "        for i in multi_sampled:\n",
    "            summand = i * (i - 1) * (2 * i + 5)\n",
    "\n",
    "            summed += summand\n",
    "            \n",
    "        # Divided by 18 and subtract from var\n",
    "        \n",
    "        var = var - summed /18\n",
    "\n",
    "                                                                        \n",
    "    #####\n",
    "    else: # Don't need to do anything if the values are not repeated\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    return var\n",
    "\n",
    "\n",
    "##################\n",
    "def Z_kendall(S,var):\n",
    "    \n",
    "    if S > 0:\n",
    "        S = S-1\n",
    "    else:\n",
    "        S = S +1\n",
    "        \n",
    "    Z = S/np.sqrt(var)\n",
    "    \n",
    "    return Z\n",
    "\n",
    "\n",
    "##################\n",
    "import scipy.stats as st\n",
    "\n",
    "def mann_kendall(data, return_all = False):\n",
    "    \n",
    "    # Calculates the s value\n",
    "    S = S_kendall(data)\n",
    "    \n",
    "    # Calculates the varience, does both repeated and non-repeated values\n",
    "    var = var_kendall(data)\n",
    "    \n",
    "    # The z value\n",
    "    Z = Z_kendall(S,var)\n",
    "    \n",
    "    # The p-value form the normal distribution\n",
    "    p_val = 2 * (1 - st.norm.cdf(abs(Z)))  # two tail test\n",
    "    # Not really sure where above comes from, but it is included in the other person function\n",
    "    # and seems to make more sense\n",
    "#     p_val = st.norm.cdf(Z)\n",
    "    \n",
    "    # In case I want to check what the z-value is\n",
    "    if return_all:\n",
    "        return S,var, Z, p_val\n",
    "    else:\n",
    "        return p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This calculation returns the total number of events per year at each grid location. \n",
    "\n",
    "def return_year_count_each_point(data):\n",
    "    phases = ['enhanced', 'suppressed', 'inactive','all']\n",
    "    storage = []\n",
    "    \n",
    "    # Looping through thye MJO phases\n",
    "    for phase in phases:\n",
    "        \n",
    "        sub_data = data.sel(mjo  = phase)\n",
    "        #  Splitting by year and counting\n",
    "        \n",
    "        # This is the start of the wet season\n",
    "        start_wet  = sub_data.where(sub_data.time.dt.month.isin([10,11,12]), drop = True)\n",
    "        start_wet_count = start_wet.groupby('time.year').count(dim = 'time')\n",
    "        # we don't want the final year, this will only give a parital year\n",
    "        number_years = len(start_wet_count.year)\n",
    "        # Slicing between the start and one from the end\n",
    "        start_wet_count = start_wet_count.isel(year = slice(0, number_years - 1))\n",
    "        \n",
    "        \n",
    "        # End of the wet season\n",
    "        end_wet  = sub_data.where(sub_data.time.dt.month.isin([1,2,3]), drop = True)\n",
    "        end_wet_count = end_wet.groupby('time.year').count(dim = 'time')\n",
    "        # Removign one so the end of the wet season can be matched with the start of the wet season\n",
    "        end_wet_count['year'] = end_wet_count.year - 1\n",
    "        \n",
    "        phase_year_count = start_wet_count + end_wet_count\n",
    "        \n",
    "        storage.append(phase_year_count)\n",
    "        \n",
    "        \n",
    "    year_count = xr.concat(storage, pd.Index(phases, name = 'mjo'))\n",
    "    \n",
    "    \n",
    "    return year_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_trend(x, t):\n",
    "    # Linear trend line of order 1\n",
    "    grad = np.polyfit(t, x, 1)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "percentiles = np.arange(0,105, 5)\n",
    "# percentiles = [60,70]\n",
    "\n",
    "storage  = []\n",
    "\n",
    "\n",
    "# Looping througth all the percentile values\n",
    "for q in percentiles:\n",
    "    print(q)\n",
    "    \n",
    "    # The climatology of this percentiule\n",
    "    climatology = precip.groupby('time.month').reduce(np.nanpercentile, q = q, dim = 'time')\n",
    "\n",
    "    # Getting the values above this threshold\n",
    "    above_q = return_extremes(phase_precip, climatology)\n",
    "\n",
    "    # Count the number of events each year above this threshold\n",
    "    count_above_q = return_year_count_each_point(above_q)\n",
    "    \n",
    "    # Finding the trend of the number of events each year\n",
    "    time_dim = 1\n",
    "    count_trend_meta = np.apply_along_axis(grid_trend, time_dim, count_above_q , t=count_above_q .year.values)\n",
    "\n",
    "    count_trend  = xr.Dataset(\n",
    "        {'trend':(('mjo','lat','lon'), count_trend_meta)},\n",
    "\n",
    "        {\n",
    "        'mjo':count_above_q.mjo.values, \n",
    "         'lat':count_above_q.lat,\n",
    "        'lon':count_above_q.lon}\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Converting to a percent per decade trend\n",
    "    count_above_q_mean = count_above_q.mean(dim = 'year')\n",
    "    count_trend_percent = (count_trend.trend * 10 / count_above_q_mean ) * 100\n",
    "    \n",
    "    \n",
    "    storage.append(count_trend_percent)\n",
    "    \n",
    "total_trend = xr.concat(storage, pd.Index(percentiles, name = 'percentile'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trend = total_trend.to_dataset(name = 'trend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:     (lat: 53, lon: 178, mjo: 4, percentile: 21)\n",
       "Coordinates:\n",
       "  * mjo         (mjo) object 'enhanced' 'suppressed' 'inactive' 'all'\n",
       "  * lat         (lat) float64 -23.0 -22.75 -22.5 -22.25 ... -10.5 -10.25 -10.0\n",
       "  * lon         (lon) float64 112.0 112.2 112.5 112.8 ... 155.8 156.0 156.2\n",
       "  * percentile  (percentile) int64 0 5 10 15 20 25 30 ... 70 75 80 85 90 95 100\n",
       "Data variables:\n",
       "    trend       (percentile, mjo, lat, lon) float64 dask.array<shape=(21, 4, 53, 178), chunksize=(1, 1, 50, 50)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trend.to_netcdf('total_trend.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_trend = xr.open_dataset('total_trend.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trend of Trends'''\n",
    "\n",
    "trend_trend_meta = np.apply_along_axis(grid_trend, 0, total_trend.trend, t=total_trend.percentile.values)\n",
    "\n",
    "trend_trend  = xr.Dataset(\n",
    "    {'trend':(('mjo','lat','lon'), trend_trend_meta)},\n",
    "    \n",
    "    {\n",
    "    'mjo':total_trend.mjo.values, \n",
    "     'lat':total_trend.lat,\n",
    "    'lon':total_trend.lon}\n",
    "\n",
    ")\n",
    "\n",
    "# Convering it to percent per decile (Use to be just percent)\n",
    "trend_trend = trend_trend * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Significance\"'''\n",
    "\n",
    "# Calculating the significance\n",
    "trend_trend_pval_meta = np.apply_along_axis(mann_kendall, 0, total_trend )\n",
    "\n",
    "trend_trend_pval  = xr.Dataset(\n",
    "    {'trend':(('mjo','lat','lon'),trend_trend_pval_meta)},\n",
    "    \n",
    "    {\n",
    "    'mjo':total_trend.mjo.values, \n",
    "     'lat':total_trend.lat,\n",
    "    'lon':total_trend.lon}\n",
    "\n",
    ")\n",
    "\n",
    "# The ocean shows up as zero, this is gettting rid of the ocean and turning them back to nans\n",
    "trend_trend_pval_land = trend_trend_pval.where(trend_trend_pval != 0 , np.nan)\n",
    "\n",
    "# Just getting the points that are significant\n",
    "trend_trend_pval_sig = trend_trend_pval_land.where(trend_trend_pval_land.trend <= 0.05)\n",
    "\n",
    "# Getting the trend points that are significant\n",
    "trend_trend_sig = trend_trend.where(\n",
    "    np.logical_and(trend_trend_pval_sig.trend >= 0 ,trend_trend_pval_sig.trend <= 0.05  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_trend_sig.sel(mjo = 'enhanced').trend.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_trend_sig.to_netcdf('trend_trend_sig.nc')\n",
    "trend_trend.to_netcdf('trend_trend.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
